{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf4e0515",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import LLM, SamplingParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ba76ce69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-19 18:52:27 [config.py:841] This model supports multiple tasks: {'reward', 'generate', 'embed', 'classify'}. Defaulting to 'generate'.\n",
      "INFO 07-19 18:52:27 [config.py:1472] Using max model len 1024\n",
      "INFO 07-19 18:52:27 [config.py:2285] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 07-19 18:52:28 [core.py:526] Waiting for init message from front-end.\n",
      "INFO 07-19 18:52:28 [core.py:69] Initializing a V1 LLM engine (v0.9.2) with config: model='llm-jp/llm-jp-3-3.7b-instruct', speculative_config=None, tokenizer='llm-jp/llm-jp-3-3.7b-instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=1024, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=llm-jp/llm-jp-3-3.7b-instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":512,\"local_cache_dir\":null}\n",
      "INFO 07-19 18:52:29 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "WARNING 07-19 18:52:29 [interface.py:382] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.\n",
      "WARNING 07-19 18:52:29 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "INFO 07-19 18:52:29 [gpu_model_runner.py:1770] Starting to load model llm-jp/llm-jp-3-3.7b-instruct...\n",
      "INFO 07-19 18:52:29 [gpu_model_runner.py:1775] Loading model from scratch...\n",
      "INFO 07-19 18:52:30 [cuda.py:284] Using Flash Attention backend on V1 engine.\n",
      "INFO 07-19 18:52:30 [weight_utils.py:292] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4af3526c33841e5b32c4236f35bae67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-19 18:52:32 [default_loader.py:272] Loading weights took 1.71 seconds\n",
      "INFO 07-19 18:52:33 [gpu_model_runner.py:1801] Model loading took 7.0483 GiB and 3.011358 seconds\n",
      "INFO 07-19 18:52:39 [backends.py:508] Using cache directory: /home/tc/.cache/vllm/torch_compile_cache/406d361875/rank_0_0/backbone for vLLM's torch.compile\n",
      "INFO 07-19 18:52:39 [backends.py:519] Dynamo bytecode transform time: 5.99 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank0]:W0719 18:52:40.653000 18216 env/lib/python3.11/site-packages/torch/_inductor/utils.py:1250] [0/0] Not enough SMs to use max_autotune_gemm mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-19 18:52:41 [backends.py:181] Cache the graph of shape None for later use\n",
      "INFO 07-19 18:52:59 [backends.py:193] Compiling a graph for general shape takes 20.01 s\n",
      "INFO 07-19 18:53:20 [monitor.py:34] torch.compile takes 26.01 s in total\n",
      "INFO 07-19 18:53:22 [gpu_worker.py:232] Available KV cache memory: -1.49 GiB\n",
      "ERROR 07-19 18:53:22 [core.py:586] EngineCore failed to start.\n",
      "ERROR 07-19 18:53:22 [core.py:586] Traceback (most recent call last):\n",
      "ERROR 07-19 18:53:22 [core.py:586]   File \"/home/tc/Github/fun/env/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 577, in run_engine_core\n",
      "ERROR 07-19 18:53:22 [core.py:586]     engine_core = EngineCoreProc(*args, **kwargs)\n",
      "ERROR 07-19 18:53:22 [core.py:586]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 07-19 18:53:22 [core.py:586]   File \"/home/tc/Github/fun/env/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 404, in __init__\n",
      "ERROR 07-19 18:53:22 [core.py:586]     super().__init__(vllm_config, executor_class, log_stats,\n",
      "ERROR 07-19 18:53:22 [core.py:586]   File \"/home/tc/Github/fun/env/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 82, in __init__\n",
      "ERROR 07-19 18:53:22 [core.py:586]     self._initialize_kv_caches(vllm_config)\n",
      "ERROR 07-19 18:53:22 [core.py:586]   File \"/home/tc/Github/fun/env/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 146, in _initialize_kv_caches\n",
      "ERROR 07-19 18:53:22 [core.py:586]     kv_cache_configs = [\n",
      "ERROR 07-19 18:53:22 [core.py:586]                        ^\n",
      "ERROR 07-19 18:53:22 [core.py:586]   File \"/home/tc/Github/fun/env/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 147, in <listcomp>\n",
      "ERROR 07-19 18:53:22 [core.py:586]     get_kv_cache_config(vllm_config, kv_cache_spec_one_worker,\n",
      "ERROR 07-19 18:53:22 [core.py:586]   File \"/home/tc/Github/fun/env/lib/python3.11/site-packages/vllm/v1/core/kv_cache_utils.py\", line 943, in get_kv_cache_config\n",
      "ERROR 07-19 18:53:22 [core.py:586]     check_enough_kv_cache_memory(vllm_config, kv_cache_spec, available_memory)\n",
      "ERROR 07-19 18:53:22 [core.py:586]   File \"/home/tc/Github/fun/env/lib/python3.11/site-packages/vllm/v1/core/kv_cache_utils.py\", line 555, in check_enough_kv_cache_memory\n",
      "ERROR 07-19 18:53:22 [core.py:586]     raise ValueError(\"No available memory for the cache blocks. \"\n",
      "ERROR 07-19 18:53:22 [core.py:586] ValueError: No available memory for the cache blocks. Try increasing `gpu_memory_utilization` when initializing the engine.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process EngineCore_0:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/tc/Github/fun/env/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/tc/Github/fun/env/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/tc/Github/fun/env/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 590, in run_engine_core\n",
      "    raise e\n",
      "  File \"/home/tc/Github/fun/env/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 577, in run_engine_core\n",
      "    engine_core = EngineCoreProc(*args, **kwargs)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tc/Github/fun/env/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 404, in __init__\n",
      "    super().__init__(vllm_config, executor_class, log_stats,\n",
      "  File \"/home/tc/Github/fun/env/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 82, in __init__\n",
      "    self._initialize_kv_caches(vllm_config)\n",
      "  File \"/home/tc/Github/fun/env/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 146, in _initialize_kv_caches\n",
      "    kv_cache_configs = [\n",
      "                       ^\n",
      "  File \"/home/tc/Github/fun/env/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 147, in <listcomp>\n",
      "    get_kv_cache_config(vllm_config, kv_cache_spec_one_worker,\n",
      "  File \"/home/tc/Github/fun/env/lib/python3.11/site-packages/vllm/v1/core/kv_cache_utils.py\", line 943, in get_kv_cache_config\n",
      "    check_enough_kv_cache_memory(vllm_config, kv_cache_spec, available_memory)\n",
      "  File \"/home/tc/Github/fun/env/lib/python3.11/site-packages/vllm/v1/core/kv_cache_utils.py\", line 555, in check_enough_kv_cache_memory\n",
      "    raise ValueError(\"No available memory for the cache blocks. \"\n",
      "ValueError: No available memory for the cache blocks. Try increasing `gpu_memory_utilization` when initializing the engine.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Engine core initialization failed. See root cause above. Failed core proc(s): {}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_8010/431515567.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m ]\n\u001b[1;32m      4\u001b[0m \u001b[0msampling_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSamplingParams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_p\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.95\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m llm = LLM(\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"llm-jp/llm-jp-3-3.7b-instruct\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mgpu_memory_utilization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Github/fun/env/lib/python3.11/site-packages/vllm/entrypoints/llm.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, task, tokenizer, tokenizer_mode, skip_tokenizer_init, trust_remote_code, allowed_local_media_path, tensor_parallel_size, dtype, quantization, revision, tokenizer_revision, seed, gpu_memory_utilization, swap_space, cpu_offload_gb, enforce_eager, max_seq_len_to_capture, disable_custom_all_reduce, disable_async_output_proc, hf_token, hf_overrides, mm_processor_kwargs, override_pooler_config, compilation_config, **kwargs)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m         \u001b[0;31m# Create the Engine (autoselects V0 vs V1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m         self.llm_engine = LLMEngine.from_engine_args(\n\u001b[0m\u001b[1;32m    272\u001b[0m             engine_args=engine_args, usage_context=UsageContext.LLM_CLASS)\n\u001b[1;32m    273\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mllm_engine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Github/fun/env/lib/python3.11/site-packages/vllm/engine/llm_engine.py\u001b[0m in \u001b[0;36mfrom_engine_args\u001b[0;34m(cls, engine_args, usage_context, stat_loggers)\u001b[0m\n\u001b[1;32m    499\u001b[0m             \u001b[0mengine_cls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mV1LLMEngine\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 501\u001b[0;31m         return engine_cls.from_vllm_config(\n\u001b[0m\u001b[1;32m    502\u001b[0m             \u001b[0mvllm_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvllm_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m             \u001b[0musage_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0musage_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Github/fun/env/lib/python3.11/site-packages/vllm/v1/engine/llm_engine.py\u001b[0m in \u001b[0;36mfrom_vllm_config\u001b[0;34m(cls, vllm_config, usage_context, stat_loggers, disable_log_stats)\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0mdisable_log_stats\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m     ) -> \"LLMEngine\":\n\u001b[0;32m--> 124\u001b[0;31m         return cls(vllm_config=vllm_config,\n\u001b[0m\u001b[1;32m    125\u001b[0m                    \u001b[0mexecutor_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mExecutor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvllm_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m                    \u001b[0mlog_stats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mdisable_log_stats\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Github/fun/env/lib/python3.11/site-packages/vllm/v1/engine/llm_engine.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, vllm_config, executor_class, log_stats, usage_context, stat_loggers, mm_registry, use_cached_outputs, multiprocess_mode)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;31m# EngineCore (gets EngineCoreRequests and gives EngineCoreOutputs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m         self.engine_core = EngineCoreClient.make_client(\n\u001b[0m\u001b[1;32m    102\u001b[0m             \u001b[0mmultiprocess_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmultiprocess_mode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0masyncio_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Github/fun/env/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\u001b[0m in \u001b[0;36mmake_client\u001b[0;34m(multiprocess_mode, asyncio_mode, vllm_config, executor_class, log_stats)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmultiprocess_mode\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0masyncio_mode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mSyncMPClient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvllm_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_stats\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mInprocClient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvllm_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_stats\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Github/fun/env/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, vllm_config, executor_class, log_stats)\u001b[0m\n\u001b[1;32m    501\u001b[0m     def __init__(self, vllm_config: VllmConfig, executor_class: type[Executor],\n\u001b[1;32m    502\u001b[0m                  log_stats: bool):\n\u001b[0;32m--> 503\u001b[0;31m         super().__init__(\n\u001b[0m\u001b[1;32m    504\u001b[0m             \u001b[0masyncio_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m             \u001b[0mvllm_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvllm_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Github/fun/env/lib/python3.11/site-packages/vllm/v1/engine/core_client.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, asyncio_mode, vllm_config, executor_class, log_stats, client_addresses)\u001b[0m\n\u001b[1;32m    401\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m                 \u001b[0;31m# Engines are managed by this client.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m                 with launch_core_engines(vllm_config, executor_class,\n\u001b[0m\u001b[1;32m    404\u001b[0m                                          \u001b[0mlog_stats\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mengine_manager\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m                                                         \u001b[0mcoordinator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Github/fun/env/lib/python3.11/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, typ, value, traceback)\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtyp\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Github/fun/env/lib/python3.11/site-packages/vllm/v1/engine/utils.py\u001b[0m in \u001b[0;36mlaunch_core_engines\u001b[0;34m(vllm_config, executor_class, log_stats, num_api_servers)\u001b[0m\n\u001b[1;32m    432\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m         \u001b[0;31m# Now wait for engines to start.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 434\u001b[0;31m         wait_for_engine_startup(\n\u001b[0m\u001b[1;32m    435\u001b[0m             \u001b[0mhandshake_socket\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m             \u001b[0maddresses\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Github/fun/env/lib/python3.11/site-packages/vllm/v1/engine/utils.py\u001b[0m in \u001b[0;36mwait_for_engine_startup\u001b[0;34m(handshake_socket, addresses, core_engines, parallel_config, cache_config, proc_manager, coord_process)\u001b[0m\n\u001b[1;32m    482\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcoord_process\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcoord_process\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexitcode\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    483\u001b[0m                 \u001b[0mfinished\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcoord_process\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoord_process\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexitcode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 484\u001b[0;31m             raise RuntimeError(\"Engine core initialization failed. \"\n\u001b[0m\u001b[1;32m    485\u001b[0m                                \u001b[0;34m\"See root cause above. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m                                f\"Failed core proc(s): {finished}\")\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Engine core initialization failed. See root cause above. Failed core proc(s): {}"
     ]
    }
   ],
   "source": [
    "prompts = [\n",
    "    \"Hello\"\n",
    "]\n",
    "sampling_params = SamplingParams(temperature=0.8, top_p=0.95)\n",
    "llm = LLM(\n",
    "    model=\"llm-jp/llm-jp-3-3.7b-instruct\",\n",
    "    gpu_memory_utilization=0.8,\n",
    "    max_model_len=1024,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2282c43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = llm.generate(prompts, sampling_params)\n",
    "\n",
    "for output in outputs:\n",
    "    prompt = output.prompt\n",
    "    generated_text = output.outputs[0].text\n",
    "    print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
